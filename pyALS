#!/usr/bin/python3
"""
Copyright 2021-2023 Salvatore Barone <salvatore.barone@unina.it>

This is free software; you can redistribute it and/or modify it under
the terms of the GNU General Public License as published by the Free
Software Foundation; either version 3 of the License, or any later version.

This is distributed in the hope that it will be useful, but WITHOUT
ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for
more details.

You should have received a copy of the GNU General Public License along with
RMEncoder; if not, write to the Free Software Foundation, Inc., 51 Franklin
Street, Fifth Floor, Boston, MA 02110-1301, USA.
"""

import sys, os, click, git, time, random
from distutils.dir_util import mkpath
from distutils.file_util import copy_file
from tqdm import tqdm
from multiprocessing import cpu_count
from src.MOP import *
from src.IAMOP import *
from src.stats import *
from src.ConfigParser import *
from src.PyModelArithInt import *
from src.ALWANNPyModelArithInt import *
from src.TbGenerator import *
from pyalslib import YosysHelper, ALSCatalog, ALSGraph, ALSRewriter, check_for_file, hamming, synthesize_at_dist
from git import RemoteProgress
from pathlib import Path

def rm_old_implementation(output_directory, files =  ".v"):
    for file in os.listdir(output_directory):
        if file.endswith(files):
            print(f"Removing old source-file {output_directory}/{file}")
            os.remove(f"{output_directory}/{file}")

def create_yshelper(ctx):
    if "yshelper" not in ctx.obj:
        print("Creating yshelper")
        ctx.obj["yshelper"] = YosysHelper()
        ctx.obj["yshelper"].load_ghdl()
        
def load_condiguration(ctx):
    if "configuration" not in ctx.obj:
        assert "configfile" in ctx.obj, "You must provide a JSON configuration file to run this command(s)"
        ctx.obj["configuration"] = ConfigParser(ctx.obj['configfile'])
        check_for_file(ctx.obj["configuration"].als_conf.lut_cache)
        
def create_alsgraph(ctx):
    if "graph" not in ctx.obj:
        assert "configuration" in ctx.obj, "You must read the JSON configuration file to run this command(s)"
        assert "yshelper" in ctx.obj, "You must create a YosysHelper object first"
        print("Graph generation...")
        ctx.obj["yshelper"].read_sources(ctx.obj["configuration"].source_hdl, ctx.obj["configuration"].top_module)
        ctx.obj["yshelper"].prep_design(ctx.obj["configuration"].als_conf.cut_size)
        ctx.obj["graph"] = ALSGraph(ctx.obj["yshelper"].design)
        ctx.obj["yshelper"].save_design("original")
        print("Done!")
        
def create_catalog(ctx):
    if "catalog" not in ctx.obj:
        assert "configuration" in ctx.obj, "You must read the JSON configuration file to run this command(s)"
        assert "yshelper" in ctx.obj, "You must create a YosysHelper object first"
        assert "graph" in ctx.obj, "You must create a ALSGraph object first"
        ctx.obj["luts_set"] = ctx.obj["yshelper"].get_luts_set()
        print(f"Performing catalog generation using {ctx.obj['ncpus']} threads. Please wait patiently. This may take time.")
        ctx.obj["catalog"] = ALSCatalog(ctx.obj["configuration"].als_conf.lut_cache, ctx.obj["configuration"].als_conf.solver).generate_catalog(ctx.obj["luts_set"], ctx.obj["configuration"].als_conf.timeout, ctx.obj['ncpus'])
        print("Done!")
        
def parse_input_weights(ctx):
    assert "configuration" in ctx.obj, "You must read the JSON configuration file to run this command(s)"
    assert "graph" in ctx.obj, "You must create a ALSGraph object first"
    if ctx.obj["configuration"].weights is not None and len(ctx.obj["configuration"].weights) != 0:
        print("Input-weight parsing...")
        ctx.obj["input_weights"] = ctx.obj["graph"].validate_pi_weights(ctx.obj["configuration"].weights)
        print("Done!")
        
def parse_output_weights(ctx):
    assert "configuration" in ctx.obj, "You must read the JSON configuration file to run this command(s)"
    assert "graph" in ctx.obj, "You must create a ALSGraph object first"
    if ctx.obj["configuration"].weights is not None and len(ctx.obj["configuration"].weights) != 0:
        print("Output-weight parsing...")
        ctx.obj["output_weights"] = ctx.obj["graph"].validate_po_weights(ctx.obj["configuration"].weights)
        print("Done!")
        
def create_problem(ctx):
    assert "configuration" in ctx.obj, "You must read the JSON configuration file to run this command(s)"
    assert "graph" in ctx.obj, "You must create a ALSGraph object first"
    if "problem" not in ctx.obj:
        ctx.obj["problem"] = MOP(ctx.obj["configuration"].top_module, ctx.obj["graph"], ctx.obj["output_weights"], ctx.obj["catalog"], ctx.obj["configuration"].error_conf, ctx.obj["configuration"].hw_conf, ctx.obj['ncpus']) if ctx.obj['dataset'] is None else IAMOP(ctx.obj["configuration"].top_module, ctx.obj["graph"], ctx.obj["output_weights"], ctx.obj["catalog"], ctx.obj["configuration"].error_conf, ctx.obj["configuration"].hw_conf, ctx.obj['ncpus'], ctx.obj['dataset'])
        ctx.obj["problem"].init()
        
def create_optimizer(ctx):
    if "optimizer" not in ctx.obj:
        print("Creating optimizer...")
        grp = ctx.obj["configuration"].variable_grouping_strategy 
        tso = ctx.obj["configuration"].transfer_strategy_objectives
        tsv = ctx.obj["configuration"].transfer_strategy_variables
        if grp is None:
            ctx.obj["optimizer"] = pyamosa.Optimizer(ctx.obj["configuration"].amosa_conf)
        elif grp in ["DRG", "drg", "random"]:
            print("Using dynamic random grouping")
            ctx.obj["optimizer"] = pyamosa.DynamicRandomGroupingOptimizer(ctx.obj["configuration"].amosa_conf)
        elif grp in ["dvg", "DVG", "dvg2", "DVG2", "differential"]:
            print(f"Using differential grouping with TSO {tso} and TSV {tsv}")
            variable_decomposition_cache = f"{ctx.obj['configuration'].output_dir}/dvg2_{tso}_{tsv}.json5"
            grouper = pyamosa.DifferentialVariableGrouping2(ctx.obj["problem"])
            if os.path.exists(variable_decomposition_cache):
                grouper.load(variable_decomposition_cache)
            else:
                grouper.run(ctx.obj["configuration"].tso_selector[tso], ctx.obj["configuration"].tsv_selector[tsv])
                grouper.store(variable_decomposition_cache)
            ctx.obj["optimizer"] = pyamosa.GenericGroupingOptimizer(ctx.obj["configuration"], grouper)
        ctx.obj["final_archive_json"] = f"{ctx.obj['configuration'].output_dir}/final_archive.json"
        ctx.obj["improve"] = None
        if os.path.exists(ctx.obj["final_archive_json"]):
            print("Using results from previous runs as a starting point.")
            ctx.obj["improve"] = ctx.obj["final_archive_json"]
    
@click.group(chain=True)
@click.option('-c', '--conf', type=click.Path(exists=True, dir_okay=False), default = None, help = "Json configuration file")
@click.option('-j', "--ncpus", type = int, help = f"Number of parallel jobs to be used turing DSE. By default, it is {cpu_count()}", default = cpu_count())
@click.option('-d', '--dataset', type=click.Path(exists=True, dir_okay=False), default = None, help = "Reference dataset, in Json format")
@click.pass_context
def cli(ctx, conf, ncpus, dataset):
    ctx.ensure_object(dict)
    ctx.obj['configfile'] = conf
    ctx.obj['ncpus'] = ncpus
    ctx.obj['dataset'] = dataset


@click.command("elab")
@click.pass_context
@click.option('-o', '--output', type=click.Path(), required = True, help = "Output path")
def elaborate(ctx, output):
    """
    Draws a k-LUT map of the given circuit
    """
    print("Performing ELAB")
    create_yshelper(ctx)
    load_condiguration(ctx)
    create_alsgraph(ctx)
    for v in ctx.obj["graph"].get_cells():
        print(v["name"], v["spec"])
    ctx.obj["graph"].save(output)


@click.command("es")
@click.pass_context
def es_synth(ctx):
    """
    Performs the catalog-based AIG-rewriting workflow until catalog generation, i.e., including cut enumeration, and
    exact synthesis of approximate cuts, but it performs neither the design space exploration phase not the rewriting.
    """
    print("Performing ES")
    create_yshelper(ctx)
    load_condiguration(ctx)
    create_alsgraph(ctx)
    create_catalog(ctx)


@click.command("als")
@click.pass_context
def als(ctx):
    """
    Performs the full catalog-based AIG-rewriting workflow, including cut enumeration, exact synthesis of approximate
    cuts, and design space exploration. It does not performs HDL generation.
    """
    print("Performing ALS")
    create_yshelper(ctx)
    load_condiguration(ctx)
    create_alsgraph(ctx)
    parse_output_weights(ctx)
    create_catalog(ctx)
    
    if ctx.obj["configuration"].output_dir != ".":
        mkpath(ctx.obj["configuration"].output_dir)
        
    create_problem(ctx)
    if not isinstance(ctx.obj["problem"], IAMOP) and not os.path.exists(f"{ctx.obj['configuration'].output_dir}/test_vectors.json"):
        print(f"Storing test vector to {ctx.obj['configuration'].output_dir}/test_vectors.json")
        ctx.obj["problem"].store_samples(f"{ctx.obj['configuration'].output_dir}/test_vectors.json")
    
    fitness_labels = ctx.obj['problem'].plot_labels()

    create_optimizer(ctx)
    init_t = time.time()
    print("AMOSA termination criterion:")
    ctx.obj["configuration"].termination_criterion.info()
    print(f"Performing AMOSA heuristic using {ctx.obj['ncpus']} threads. Please wait patiently. This may take time.")
    ctx.obj["optimizer"].run(ctx.obj["problem"], termination_criterion = ctx.obj["configuration"].termination_criterion, improve = ctx.obj["improve"])
    dt = time.time() - init_t
    ctx.obj["optimizer"].archive.write_json(ctx.obj["final_archive_json"])
    ctx.obj["optimizer"].archive.plot_front(ctx.obj['problem'], f"{ctx.obj['configuration'].output_dir}/pareto_front.pdf", ctx.obj["configuration"].top_module, fitness_labels)
    
    print(f"AMOSA heuristic completed in {dt} seconds")
    hours = int(ctx.obj["optimizer"].duration / 3600)
    minutes = int((ctx.obj["optimizer"].duration - hours * 3600) / 60)
    print(f"Took {hours} hours, {minutes} minutes")
    print(f"Cache hits: {ctx.obj['problem'].cache_hits} over {ctx.obj['problem'].total_calls} evaluations.")
    print(f"{len(ctx.obj['problem'].cache)} cache entries collected")


@click.command('hdl')
@click.option('-o', '--output', type=click.Path(file_okay=False, dir_okay=True), default = None, help = "Output path")
@click.pass_context
def generate_verilog(ctx, output):
    """
    Performs the rewriting step of the catalog-based AIG-rewriting workflow to generate HDL, starting from the results of a previous run of the "als" command.
    """
    print("Generating HDL")
    create_yshelper(ctx)
    load_condiguration(ctx)
    if output is None:
        output = ctx.obj['configuration'].output_dir
        
    create_alsgraph(ctx)
    parse_output_weights(ctx)
    create_catalog(ctx)
    create_problem(ctx)
    if output != ".":
        mkpath(output)
    
    problem = MOP(ctx.obj["configuration"].top_module, ctx.obj["graph"], None, ctx.obj["catalog"], ctx.obj["configuration"].error_conf, ctx.obj["configuration"].hw_conf, ctx.obj["ncpus"])
    problem.init()
    create_optimizer(ctx)
    
    print("Reading the Pareto front.")
    ctx.obj["optimizer"].archive = pyamosa.Pareto()
    ctx.obj["optimizer"].archive.read_json(problem, ctx.obj["final_archive_json"])
    pareto_set = ctx.obj["optimizer"].archive.get_set()
    rm_old_implementation(output)
    print("Performing AIG-rewriting.")
    rewriter = ALSRewriter(ctx.obj["yshelper"], problem)
    rewriter.generate_hdl(pareto_set, output)
        
    resource_dir = os.path.dirname(os.path.abspath(__file__))
    for resource in [
        "resources/.synopsys_dc.setup",
        "resources/do_synth.sh",
        "resources/do_synth.tcl",
        "resources/do_fpga_synth.sh",
        "resources/fpga_synth.tcl",
        "resources/gscl45nm.db",
        "resources/gscl45nm.v",
        "resources/do_sim.sh",
        "resources/do_sim.tcl",
        "resources/do_pwr.tcl"
    ]:
        copy_file(f"{resource_dir}/{resource}", output)
    print(f"All done! Take a look at {output}!")


@click.command("sw")
@click.option('-o', '--output', type=click.Path(file_okay=False, dir_okay=True), default = None, help = "Output path")
@click.option("--altconf", type=click.Path(exists=True, dir_okay=False), help = "Optional alternative configuration file (for signals' weights)", default = None)
@click.option("-e", "--exact", is_flag = True, help = "Generates the exact implementation only")
@click.option("-a", "--alwann", is_flag = True, help = "Enable the weight-tuning approach. See below.")
@click.pass_context
def generate_sw(ctx, output, altconf, exact, alwann):
    """
    Generates software models of twp-inputs-one-output arithmetic circuits resulting from the 'als' command, for GPU software simulations.
    You can select which models to be generated using the available options.

    If the "-a"/"--alwann" option is enabled, the weight-tuning approach from [1] is used to tune the behavior of the multiplier.
    
    [1] Mrazek, Vojtech, Zdenek Vasicek, Lukas Sekanina, Muhammad Abdullah Hanif, e Muhammad Shafique. 
        "ALWANN: Automatic Layer-Wise Approximation of Deep Neural Network Accelerators without Retraining"
        2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), novembre 2019, 1–8. 
        https://doi.org/10.1109/ICCAD45719.2019.8942068.
    
    """
    print("Generating software models")
    create_yshelper(ctx)
    load_condiguration(ctx)
    if output is None:
        output = ctx.obj['configuration'].output_dir
    create_alsgraph(ctx)
    create_catalog(ctx)
    configuration = ConfigParser(altconf) if altconf is not None else ctx.obj["configuration"]
    original_weights = copy.deepcopy(ctx.obj["configuration"].weights)
    ctx.obj["configuration"].weights = configuration.weights    
    parse_input_weights(ctx)
    parse_output_weights(ctx)
    create_problem(ctx)
    
    problem = ctx.obj['problem'] if isinstance(ctx.obj['problem'], MOP) else MOP(ctx.obj["configuration"].top_module, ctx.obj["graph"], None, ctx.obj["catalog"], ctx.obj["configuration"].error_conf, ctx.obj["configuration"].hw_conf, ctx.obj["ncpus"])
    create_optimizer(ctx)
    if not exact:
        print("Reading the Pareto front.")
        ctx.obj["optimizer"].archive = pyamosa.Pareto()
        ctx.obj["optimizer"].archive.read_json(problem, ctx.obj["final_archive_json"])
        pareto_set = ctx.obj["optimizer"].archive.get_set()
        print(f"{len(pareto_set)} solutions read from {ctx.obj['final_archive_json']}")
    else:
        print("Generating the fake Pareto front.")
        pareto_set = [[0] * problem.n_vars]
    generator = ALWANNPyModelArithInt(ctx.obj["yshelper"], problem, ctx.obj["configuration"].weights) if alwann else PyModelArithInt(ctx.obj["yshelper"], problem, ctx.obj["configuration"].weights)
    
    mkpath(output)
    rm_old_implementation(output, ".py")
    rm_old_implementation(output, ".c")
    rm_old_implementation(output, ".h")
    generator.generate(ctx.obj["configuration"].top_module, pareto_set, output)
    ctx.obj["configuration"].weights = original_weights
    print(f"All done! Take a look at {output}!")
    
@click.command("tb")
@click.option('-o', '--output', type=click.Path(file_okay=False, dir_okay=True), default = None, help = "Output path")
@click.option("--delay", type=int, default = 10, help = "Simulation delay")
@click.option("--nvec", type=int, default = None, help = "Number of (random) test vectors (uniformly drawn with repetition).")
@click.pass_context
def generate_tb(ctx, output, delay, nvec):
    """ Generate testbench and scritps files for vectored power estimation """
    print("Generating testbench")
    create_yshelper(ctx)
    load_condiguration(ctx)
    create_alsgraph(ctx)
    create_catalog(ctx)
    if output is None:
        output = f"{ctx.obj['configuration'].output_dir}" 

    ctx.obj["configuration"].error_conf.n_vectors = nvec
    problem = MOP(ctx.obj["configuration"].top_module, ctx.obj["graph"], None, ctx.obj["catalog"], ctx.obj["configuration"].error_conf, ctx.obj["configuration"].hw_conf, ctx.obj["ncpus"])
    problem.init()

    mkpath(output)
    #create_optimizer(ctx)
    TbGenerator(ctx.obj["yshelper"], problem, delay).generate(f"{output}/tb.v", nvec)
    
    resource_dir = os.path.dirname(os.path.abspath(__file__))
    for resource in [
        "resources/.synopsys_dc.setup",
        "resources/do_synth.sh",
        "resources/do_synth.tcl",
        "resources/do_fpga_synth.sh",
        "resources/fpga_synth.tcl",
        "resources/gscl45nm.db",
        "resources/gscl45nm.v",
        "resources/do_sim.sh",
        "resources/do_sim.tcl",
        "resources/do_pwr.tcl"
    ]:
        copy_file(f"{resource_dir}/{resource}", output)
    print(f"All done! Take a look at {output}!")
    
@click.command("metrics")
@click.option("--altconf", type=click.Path(exists=True, dir_okay=False), help = "Optional alternative configuration file (for signals' weights)", default = None)
@click.option('-o', '--output', type=click.Path(file_okay=True, dir_okay=False), default = None, help = "Output path")
@click.pass_context
def fitnesses(ctx, altconf, output):
    """
    Computes the all the builtin metrics (both error and hardware) for points coming from a given Pareto front.
    """
    print("Performing fitnesses computation")
    create_yshelper(ctx)
    load_condiguration(ctx)
    create_alsgraph(ctx)
    
    if output is None:
        output = f"{ctx.obj['configuration'].output_dir}/metrics.csv"
    
    configuration = ConfigParser(altconf) if altconf is not None else ctx.obj["configuration"]
    original_weights = copy.deepcopy(ctx.obj["configuration"].weights)
    ctx.obj["configuration"].weights = configuration.weights   
    parse_input_weights(ctx)
    parse_output_weights(ctx)
    create_catalog(ctx)
    
    create_problem(ctx)
    
    create_optimizer(ctx)
    ctx.obj["optimizer"].archive = pyamosa.Pareto()
    ctx.obj["optimizer"].archive.read_json(ctx.obj["problem"], ctx.obj["final_archive_json"])
    
    print("Computing the full characterization of the Pareto front.")
    archive = [{"x": list(s)} | ctx.obj["problem"].evaluate_ffs(s) for s in tqdm(ctx.obj["optimizer"].archive.get_set(), desc="Please wait...", bar_format="{desc:40} {percentage:3.0f}% |{bar:60}{r_bar}{bar:-10b}")]
    fitness_labels = list(ctx.obj["problem"].error_labels.values()) + list(ctx.obj["problem"].hw_labels.values())
    original_stdout = sys.stdout
    row_format = "{:};" + "{:};" * ctx.obj["problem"].num_of_objectives + "{:};" * ctx.obj["problem"].num_of_variables
    with open(output, "w") as file:
        sys.stdout = file
        print(row_format.format("", *fitness_labels, *[f"x{i}" for i in range(ctx.obj["problem"].num_of_variables)]))
        for i, s in enumerate(archive):
            print(row_format.format(i, *s["f"], *s["x"]))
    sys.stdout = original_stdout
    ctx.obj["configuration"].weights = original_weights
    print(f"All done! Take a look at {output}!")
    
cli.add_command(elaborate)
cli.add_command(es_synth)
cli.add_command(als)
cli.add_command(generate_verilog)
cli.add_command(generate_tb)
cli.add_command(generate_sw)
cli.add_command(fitnesses)
    
@click.command('clean')
@click.option('--catalog', type = str, default = None, help = 'Path of the LUT-catalog cache file. If specificed, the one from the configuration file is ignored.')
@click.pass_context
def clean(ctx, catalog):
    """Performs a sanity check of the catalog """
    if ctx.obj['configfile'] is None:
        assert catalog is not None, "You must specify the path of the LUT-catalog cache file, or a JSON configuration file"
        check_for_file(catalog)
        cache = ALSCatalogCache(catalog)
    else:    
        load_condiguration(ctx)
        cache = ALSCatalogCache(ctx.obj["configuration"].als_conf.lut_cache)
    count = 0
    for s in cache.get_all_exact_luts():
        exact_synth_spec, exact_S, exact_P, exact_out_p, exact_out, exact_depth = s
        gates = len(exact_S[0])
        distance = 0
        for spec in cache.get_approx_luts(exact_synth_spec):
            ax_synth_spec, ax_S, ax_P, ax_out_p, ax_out, ax_depth = spec
            dist = hamming(exact_synth_spec, ax_synth_spec)
            if len(ax_S[0]) >= gates or dist < distance:
                cache.del_lut(exact_synth_spec, dist)
                count += 1
            else:
                gates = len(ax_S[0])
                distance = dist
    # search for complemented specifications
    for x in cache.get_all_exact_luts():
        x_synth_spec, _, _, _, _, _ = x
        for y in cache.get_all_exact_luts():
            y_synth_spec, _, _, _, _, _ = y
            if x_synth_spec == negate(y_synth_spec):
                cache.del_spec(y_synth_spec)
                count += 1
    print(f"Deleted {count} instances")


@click.command("expand")
@click.option('--catalog', type = str, default = None, help = 'Path of the LUT-catalog cache file. If specificed, the one from the configuration file is ignored.')
@click.pass_context
def expand(ctx, catalog):
    """ Attempts catalog expansion """
    if ctx.obj['configfile'] is None:
        assert catalog is not None, "You must specify the path of the LUT-catalog cache file, or a JSON configuration file"
        check_for_file(catalog)
        cache = ALSCatalogCache(catalog)
    else:    
        load_condiguration(ctx)
        cache = ALSCatalogCache(ctx.obj["configuration"].als_conf.lut_cache)
    # try to complete any incomplete path in the catalog
    luts_to_be_synthesized = set()
    for x in cache.get_all_exact_luts():
        ex_spec, ex_dist, ex_synth_spec, ex_S, ex_P, ex_p, ex_out, ex_depth = x
        axspect = cache.get_approx_luts(ex_spec)
        ax_synth_spec, ax_S, ax_P, ax_out_p, ax_out, ax_depth = axspect[-1]
        gates = len(ax_S[0])
        distance = hamming(ex_spec, ax_synth_spec)
        if gates > 0:
            print(f"Incomplete catalog found for spec {ex_spec}. Synthesis will start from Hamming distance {distance + 1}")
            luts_to_be_synthesized.add((ex_spec, gates, distance + 1))
    luts_to_be_synthesized = list(luts_to_be_synthesized)
    random.shuffle(luts_to_be_synthesized)
    luts_sets = list_partitioning(luts_to_be_synthesized, cpu_count())
    args = [[catalog, lut_set, 60000, ALSConfig.Solver.Boolector] for lut_set in luts_sets]
    with Pool(cpu_count()) as pool:
        ax_added = pool.starmap(synthesize_at_dist, args)
    print(f"{sum(ax_added)} new approximate LUTs inserted in the catalog cache while attempting catalog completion")
    # for each lut in the catalog, we add the synthesized approximate lut as exact lut at distance 0, if they do not belong to the catalog
    # this improves the hit-rate for exact luts
    exact_added = 0
    luts_to_be_synthesized = set()
    for x in cache.get_all_luts():
        x_spec, x_dist, x_synth_spec, x_S, x_P, x_p, x_out, x_depth = x
        if cache.get_lut_at_dist(x_synth_spec, 0) is None:
            cache.add_lut(x_synth_spec, 0, x_synth_spec, x_S, x_P, x_p, x_out, x_depth)
            if x_synth_spec not in luts_to_be_synthesized and negate(x_synth_spec) not in luts_to_be_synthesized:
                luts_to_be_synthesized.add(x_synth_spec)
            exact_added += 1
    print(f"{exact_added} new exact LUTs inserted in the catalog cache")
    luts_to_be_synthesized = list(luts_to_be_synthesized)
    random.shuffle(luts_to_be_synthesized)
    luts_sets = list_partitioning(luts_to_be_synthesized, cpu_count())
    args = [[catalog, lut_set, 60000, ALSConfig.Solver.Boolector] for lut_set in luts_sets]
    with Pool(cpu_count()) as pool:
        ax_added = pool.starmap(synthesize, args)
    print(f"{sum(ax_added)} new approximate LUTs inserted in the catalog cache")


@click.command("query")
@click.option('--catalog', type = str, default = None, help = 'Path of the LUT-catalog cache file. If specificed, the one from the configuration file is ignored.')
@click.option("--spec",  type=str,    required = True, help="LUT specification to search")
@click.option("--dist",  type=str,                     help="distance", default="0")
@click.option("--neg",  is_flag=True,                  help="Search for complemented spec")
@click.pass_context
def query(ctx, catalog, spec, dist, neg):
    """ Query the catalog for a specific lut implementation """
    if ctx.obj['configfile'] is None:
        assert catalog is not None, "You must specify the path of the LUT-catalog cache file, or a JSON configuration file"
        check_for_file(catalog)
        cache = ALSCatalogCache(catalog)
    else:    
        load_condiguration(ctx)
        cache = ALSCatalogCache(ctx.obj["configuration"].als_conf.lut_cache)
    x = cache.get_lut_at_dist(negate(spec) if neg else spec, dist)
    if x is None:
        print(f"{spec}@{dist} not in the catalog cache")
    else:
        print(x)


@click.command("stats")
@click.option('--catalog', type = str, default = None, help = 'Path of the LUT-catalog cache file. If specificed, the one from the configuration file is ignored.')
@click.option("--gates",                   is_flag = True,  help="histogram of functions w.r.t. the number of AIG nodes")
@click.option("--power-gates",             is_flag = True,  help="box-and-whiskers plot of switching activity w.r.t. the number of AIG nodes")
@click.option("--power-truth",             is_flag = True,  help="box-and-whiskers plot of switching activity w.r.t. the truth-density")
@click.option("--power-truth-k", type=int,                  help="box-and-whiskers plot of switching activity w.r.t. the truth-density for k-luts", default = None)
@click.pass_context
def stats(ctx, catalog, gates, power_gates, power_truth, power_truth_k):
    """ Compute statistics on a given catalog """
    if catalog is None:
        load_condiguration(ctx)
        catalog = ctx.obj["configuration"].als_conf.lut_cache
    elif not os.path.exists(catalog):
            print(f"{catalog}: no such file.")
            exit()
    if gates:
        gates_histogram(catalog)
    if power_gates:
        power_gates_boxplot(catalog)
    if power_truth:
        power_truth_boxplot(catalog)
    if power_truth_k is not None:
        power_truth_k_boxplot(power_truth_k)


cli.add_command(clean)
cli.add_command(expand)
cli.add_command(query)
cli.add_command(stats)

def git_updater():
    try:
        print("Checking for updates...")
        restart_needed = False
        repo = git.Repo(os.path.dirname(os.path.realpath(__file__)))
        for fetch_info in repo.remotes.origin.fetch(progress=RemoteProgress()):
            print(f"Updated {fetch_info.ref} to {fetch_info.commit}")

        local_head = repo.heads[0].commit
        remote_head = repo.remotes.origin.refs[0].commit
        print(f"Local commit: {local_head}")
        print(f"Last remote commit: {remote_head}")

        for fetch_info in repo.remotes.origin.pull(repo.heads[0], progress=RemoteProgress()):
            print(f"Updated {fetch_info.ref} to {fetch_info.commit}")
            if fetch_info.commit != local_head:
                restart_needed = True
                print(f"Local head moved to {local_head}. The program will be restarted.")

        print("Checking for updates in submodules...")
        for submodule in repo.submodules:
            for fetch_info in submodule.update(init = True, recursive = True):
                print(f"Updated {fetch_info.ref} to {fetch_info.commit}")
                if fetch_info.commit != local_head:
                    restart_needed = True
                    print(f"Local head moved to {local_head}. The program will be restarted.")

        return restart_needed
    except git.exc.GitCommandError as e:
        print(e)
        return False

if __name__ == '__main__':
    if git_updater():
        os.execv(sys.argv[0], sys.argv)
    else:
        cli()
